Dynamic LLM-Powered Multi-Strategy SAS Conversion (v3 - Detailed)1. Philosophy: The "AI Code Archaeologist"(Same as before) ...meticulously analyzing an artifact... selects specialized tools... dynamically focuses... precise transformations... detailed annotations...2. Core Concept: Adaptive Multi-Strategy Execution(Same as before) ...LLM to dynamically choose the best strategy... focus the context... iterative refinement... richer, explanatory output.3. Key Principles (Evolved):LLM Strategy Advisor: Recommends conversion approach.Deterministic Pre-Splitting: Handles initial breakdown of very large files.Dynamic Context Focusing: LLM isolates relevant context per chunk.Confidence-Driven Iterative Refinement: Repeats refinement on low-confidence sections.LLM-Generated Annotations: Provides explanations alongside code.Managed State: Explicit handling of context and intermediate results within Lambda memory.4. Dynamic Multi-Strategy Lambda Workflow (Conceptual State Flow):[UI Request (SAS Code)] --> [API Gateway] --> [Lambda Function State Init]
                                                       |
+------------------------------------------------------+------------------------------------------------------------+
| 0a. Deterministic Pre-processing & Splitting       | `state = {sas_super_chunks: [...], overall_context: {},   |
|     - IF code is VERY large, Split into Super-Chunks |            refined_pyspark_sections: [], annotations: []}`|
|     - Initialize `state.overall_context` (empty)   |                                                            |
+----------------------|-----------------------------+                                                            |
                       |                                                                                          |
+----------------------v-----------------------------+                                                            |
| 0b. LLM Strategy Advisor                           | `state.strategy = llm_response`                            |
|     - Send sample/summary of SAS to LLM            |                                                            |
|     - Prompt Example: "Analyze SAS complexity...   |                                                            |
|       Recommend Strategy: [Simple|Standard|Deep]"  |                                                            |
|     - Lambda selects strategy based on LLM response|                                                            |
+----------------------|-----------------------------+------------------------------------------------------------+
                       |                                                                                          |
+----------------------v-----------------------------+------------------------------------------------------------+
| LOOP: For each `super_chunk` in `state.sas_super_chunks` |                                                            |
| +------------------------------------------------+ |                                                            |
| | Pass 1: LLM - Structural Analysis (within SC)  | | `sc_logical_chunks = llm_response`                         |
| |   - Prompt Example: "Analyze SAS in this super-| |                                                            |
| |     chunk. Output JSON list of logical chunks  | |                                                            |
| |     (DATA/PROC/MACRO) with start/end lines &   | |                                                            |
| |     key input/output datasets/macros used."    | |                                                            |
| +-------------------|----------------------------+ |                                                            |
|                     |                              |                                                            |
| +-------------------v----------------------------+ |                                                            |
| | Pass 2: LLM/Code - Context Mapping (within SC) | | `sc_chunk_context_map = map_context(sc_logical_chunks)`  |
| |   - Map detailed context for `sc_logical_chunks`| |                                                            |
| +-------------------|----------------------------+ |                                                            |
|                     |                              |                                                            |
| | LOOP: For each `logical_chunk` in `sc_logical_chunks`|                                                        |
| | +----------------------------------------------+ |                                                            |
| | | Pass 2.5: LLM - Dynamic Context Focusing     | | `focused_context = llm_response`                           |
| | |   - Prompt Example: "Given Overall Context & | |                                                            |
| | |     upcoming logical_chunk code, extract     | |                                                            |
| | |     minimal relevant context (JSON subset)." | |                                                            |
| | +-----------------|----------------------------+ |                                                            |
| |                   |                              |                                                            |
| | +-----------------v----------------------------+ |                                                            |
| | | Pass 3: LLM - Focused Conv. & Annotation     | | `pyspark_chunk, chunk_annotations = llm_response`          |
| | |   - Prompt Example: "Using focused_context, | | `state.annotations.extend(chunk_annotations)`              |
| | |     convert SAS logical_chunk to PySpark.    | |                                                            |
| | |     Output JSON {pyspark_code: ...,          | |                                                            |
| | |     annotations: [{sas_lines:..., note:...}]}| |                                                            |
| | +----------------------------------------------+ |                                                            |
| | END LOOP (Logical Chunks)                      | |                                                            |
| +------------------------------------------------+ |                                                            |
| | Pass 4a: LLM - Confidence-Driven Refinement    | | `refined_section, confidence = llm_response`               |
| |   - Assemble PySpark for this SC               | | IF `confidence < threshold`: REPEAT Pass 4a (add detail) |
| |   - Prompt Example: "Refine this PySpark section| | `state.refined_pyspark_sections.append(refined_section)` |
| |     for correctness/efficiency. Provide overall| |                                                            |
| |     confidence score (1-5) & refinement notes."| |                                                            |
| +------------------------------------------------+ |                                                            |
| | Update & Summarize Overall Context             | | `state.overall_context = update_context(...)`            |
| |   - Add outputs from this SC's chunks          | | IF `context_too_large`: `state.overall_context = summarize()`|
| +------------------------------------------------+ |                                                            |
| END LOOP (Super Chunks)                          |                                                            |
+------------------------------------------------------+------------------------------------------------------------+
                                                       |
+------------------------------------------------------+
| Pass 4b: Final Assembly & Annotation Aggregation     | `final_pyspark = join(state.refined_pyspark_sections)`   |
|   - Concatenate `state.refined_pyspark_sections`     | `final_output = {pyspark: final_pyspark,                 |
|   - Aggregate `state.annotations`                    |                  annotations: state.annotations}`        |
|   - Perform basic linting/syntax check (non-LLM)     |                                                            |
+----------------------|-------------------------------+
                       |
         [API Gateway] --> [UI Response (final_output, Warnings)]
5. Detailed Pass Descriptions (Evolved & More Detail):Pass 0a: Deterministic Pre-processing & Splitting:Input: Full SAS code string (potentially very large).Logic: Uses robust regex (e.g., ^\s*(PROC|DATA|%MACRO)\s+.*; on new lines) or a simple state machine parser to split the code into Super-Chunk strings. Aims for chunks ideally under a target token count (e.g., 50k tokens, model dependent).Output: state.sas_super_chunks (list of strings). state.overall_context initialized as an empty dictionary/object.Pass 0b: LLM Strategy Advisor:Input: First N lines or a representative sample of the SAS code.Prompt Detail: "Analyze the complexity of the provided SAS code sample. Consider macro usage intensity, nesting depth, data step complexity (implicit loops, merges, arrays), PROC types used, and apparent dependencies. Recommend one strategy: 'Simple Direct' (minimal passes), 'Standard Multi-Pass' (balanced), 'Deep Macro Analysis' (extra focus on macro expansion/translation). Explain your reasoning briefly."Output: state.strategy (string). Lambda logic will adjust subsequent passes based on this (e.g., skipping passes, adding macro-specific prompts).Pass 1: LLM - Structural Analysis (within SC):Input: A single super_chunk string.Prompt Detail: "Analyze the following SAS code block. Identify all distinct logical units (DATA steps, PROC steps, defined %MACROs). Output a JSON list where each item represents a unit and includes: { 'type': 'DATA'|'PROC'|'MACRO', 'name': 'step/proc/macro_name', 'start_line': #, 'end_line': #, 'inputs': ['dataset/macro_used'], 'outputs': ['dataset/macro_created'] }."Output: sc_logical_chunks (list of JSON objects for the current super-chunk).Pass 2: LLM/Code - Context Mapping (within SC):Input: sc_logical_chunks.Logic: Primarily code-based analysis of the JSON from Pass 1 and potentially cross-referencing with state.overall_context to map dependencies more accurately. May involve light LLM calls for ambiguous cases.Output: sc_chunk_context_map (e.g., a dictionary mapping logical chunk ID/name to its detailed context requirements).Pass 2.5: LLM - Dynamic Context Focusing:Input: The code of the next logical_chunk to be processed, state.overall_context (potentially large).Prompt Detail: "Review the Overall Context (JSON below) and the upcoming SAS code snippet (below). Extract and return only the subset of the Overall Context (as JSON) that is essential for accurately converting the upcoming SAS code snippet. Include relevant dataset schemas, macro definitions, and key variable states mentioned."Output: focused_context (JSON object, smaller subset of overall_context).Pass 3: LLM - Focused Context Conversion & Annotation:Input: logical_chunk SAS code, focused_context JSON.Prompt Detail: "Given the following focused_context (JSON): {...}. Convert the sas_code (below) into efficient PySpark (using DataFrame API primarily). Output a JSON object: { 'pyspark_code': '...', 'annotations': [ { 'sas_lines': [#,#], 'pyspark_lines': [#,#], 'note': 'Explanation for complex translation/assumption', 'severity': 'Info'|'Warning' } ] }. Add annotations for non-trivial logic, SAS-specific features, or potential ambiguities."Output: JSON object with pyspark_chunk string and chunk_annotations list. Annotations are appended to state.annotations.Pass 4a: LLM - Confidence-Driven Iterative Refinement:Input: Assembled PySpark code string for the current super_chunk.Prompt Detail (Initial): "Review and refine the following PySpark code section for correctness against SAS semantics, Spark efficiency (avoid anti-patterns), and style. Add/improve comments. Provide an overall confidence score (integer 1-5, 5=high) for this section's conversion quality and list any specific areas of uncertainty or suggestions as refinement notes."Output (Initial): refined_section string, confidence score (int), refinement_notes (string/list).Logic (Iteration): IF confidence < threshold (e.g., 4): Construct a new prompt for a second refinement call, incorporating the refinement_notes: "Confidence was low ({confidence}/5) in the previous review, particularly regarding: '{refinement_notes}'. Please focus on refining these specific areas in the provided PySpark section." -> Call LLM again.Final Output (for SC): The final refined_section string for this super-chunk.Context Update & Summarization:Logic: After processing a super-chunk, update state.overall_context (in-memory dictionary) with outputs (new datasets, macros) identified in Pass 1/2 for that SC. Check size (e.g., token count estimate or object size).Summarization Trigger: IF context_too_large:Input: state.overall_context (large JSON/dict).Prompt: "Summarize the key information (active datasets, schemas, macro definitions) in the provided context JSON concisely, preserving essential details for downstream SAS conversion steps. Output the summary as compact JSON."Output: Replace state.overall_context with the summarized version.Pass 4b: Final Assembly & Annotation Aggregation:Input: state.refined_pyspark_sections (list of strings), state.annotations (list of annotation objects).Logic: Concatenate PySpark strings. Perform basic Python linting (e.g., using pyflakes or ast.parse within Lambda if feasible, or just basic regex checks). Structure annotations (e.g., group by severity or super-chunk).Output: Final JSON payload containing the full PySpark script string and the structured list of annotations.6. "Cool" Factors & Benefits: (Same as before)7. Risks & Limitations (Amplified & More Specific):Timeout Risk: Remains primary risk. Increased number of LLM calls (0b, 2.5, context summary, potential 4a repeats) makes hitting the 15-min limit highly probable for very large/complex inputs.Latency & Cost: Significantly higher due to more LLM calls. Context focusing (2.5) and summarization add overhead.Complexity: High implementation complexity, especially managing state, dynamic strategies, iterative refinement loops, and error handling across passes.LLM Reliability: Heavy reliance on LLM for nuanced tasks (strategy, focusing, confidence). Prompt engineering is critical and may require extensive tuning. Errors in meta-tasks are hard to debug.Context Fidelity: Context summarization is lossy. Dynamic focusing (2.5) relies on the LLM correctly identifying all relevant context; missing something could break conversion.Error Handling Strategy (Detail Needed): How are errors handled if an LLM call fails or returns invalid output in any pass? Retry logic? Fail fast for the super-chunk? Attempt to continue with degraded context? This needs a defined strategy (e.g., retry once, then log error and attempt to proceed if possible, marking affected sections).